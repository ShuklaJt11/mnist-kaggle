{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_labels = mnist_train_df['label'].values\n",
    "mnist_train_data = torch.Tensor(mnist_train_df.drop('label', axis=1).values.reshape(-1, 1, 28, 28))\n",
    "mnist_train = []\n",
    "\n",
    "for i in range(len(mnist_train_data)):\n",
    "    mnist_train.append([mnist_train_data[i] / 255., mnist_train_labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(42000, torch.Size([1, 28, 28]), 1)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "sample, label = mnist_train[0]\n",
    "len(mnist_train), sample.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(n, val_pct):\n",
    "    n_val = int(val_pct * n)\n",
    "    idxs = np.random.permutation(n)\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs, val_idxs = split_indices(len(mnist_train_data), 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "val_sampler = SubsetRandomSampler(val_idxs)\n",
    "val_loader = DataLoader(mnist_train, batch_size=batch_size, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 1, 28, 28]) torch.Size([128]) tensor([5, 0, 6, 9, 5])\n"
    }
   ],
   "source": [
    "for sample, label in val_loader:\n",
    "    print(sample.shape, label.shape, label[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase (nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        if 'val_loss' in result:\n",
    "            print(\"Epoch [{}], train_loss: {:.5f}, val_loss: {:.5f}, val_acc: {:.5f}\".format(\n",
    "                epoch + 1, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        else:\n",
    "            print(\"Epoch [{}], train_loss: {:.5f}\".format(\n",
    "                epoch + 1, result['train_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit_one_cycle(epochs, lr, model, train_loader, val_loader=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        result = {}\n",
    "        if val_loader is not None:\n",
    "            result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_channel, output_channel, stride=1, kernel_size=3):\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv2d(input_channel, output_channel[0], kernel_size=kernel_size, stride=stride, padding=kernel_size//2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(output_channel[0], output_channel[1], kernel_size=kernel_size, stride=stride, padding=kernel_size//2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2)\n",
    "    )\n",
    "    return block\n",
    "\n",
    "class MnistConvolutionalNeuralNetwork (ImageClassificationBase):\n",
    "    def __init__(self, input_channels, hidden_layers, output_size):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        layers.append(conv_block(input_channels, [32, 64]))\n",
    "        layers.append(conv_block(64, [128, 256]))\n",
    "\n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "        layers.append(nn.Linear(256 * 7 * 7, hidden_layers[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        self.connections = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.connections(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MnistConvolutionalNeuralNetwork(\n  (connections): Sequential(\n    (0): Sequential(\n      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU()\n      (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU()\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (1): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU()\n      (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU()\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (2): Flatten()\n    (3): Linear(in_features=12544, out_features=256, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=256, out_features=128, bias=True)\n    (6): ReLU()\n    (7): Linear(in_features=128, out_features=32, bias=True)\n    (8): ReLU()\n    (9): Linear(in_features=32, out_features=10, bias=True)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "input_channel = 1\n",
    "hidden_layers = [256, 128, 32]\n",
    "output_size = 10\n",
    "model = MnistConvolutionalNeuralNetwork(input_channel, hidden_layers, output_size)\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch [1], train_loss: 0.35431, val_loss: 0.09437, val_acc: 0.97069\nEpoch [2], train_loss: 0.06362, val_loss: 0.05753, val_acc: 0.98430\nEpoch [3], train_loss: 0.04136, val_loss: 0.05516, val_acc: 0.98269\nEpoch [4], train_loss: 0.03210, val_loss: 0.05022, val_acc: 0.98627\nEpoch [5], train_loss: 0.02338, val_loss: 0.06172, val_acc: 0.98411\nEpoch [6], train_loss: 0.02111, val_loss: 0.04966, val_acc: 0.98525\nEpoch [7], train_loss: 0.01731, val_loss: 0.04901, val_acc: 0.98672\nEpoch [8], train_loss: 0.01645, val_loss: 0.05997, val_acc: 0.98556\nEpoch [9], train_loss: 0.01169, val_loss: 0.06393, val_acc: 0.98554\nEpoch [10], train_loss: 0.01165, val_loss: 0.04698, val_acc: 0.98857\nEpoch [11], train_loss: 0.00838, val_loss: 0.05407, val_acc: 0.98660\nEpoch [12], train_loss: 0.01094, val_loss: 0.06032, val_acc: 0.98572\nEpoch [13], train_loss: 0.00989, val_loss: 0.05311, val_acc: 0.98686\nEpoch [14], train_loss: 0.01081, val_loss: 0.05147, val_acc: 0.98970\nEpoch [15], train_loss: 0.00647, val_loss: 0.05815, val_acc: 0.98745\nEpoch [16], train_loss: 0.00714, val_loss: 0.05853, val_acc: 0.98643\nEpoch [17], train_loss: 0.00910, val_loss: 0.06467, val_acc: 0.98790\nEpoch [18], train_loss: 0.00704, val_loss: 0.06998, val_acc: 0.98719\nEpoch [19], train_loss: 0.00606, val_loss: 0.05307, val_acc: 0.98790\nEpoch [20], train_loss: 0.00498, val_loss: 0.06300, val_acc: 0.98845\n"
    }
   ],
   "source": [
    "epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "history = fit_one_cycle(epochs, lr, model, train_loader, val_loader=val_loader, opt_func=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch [1], train_loss: 0.00232, val_loss: 0.05180, val_acc: 0.99062\nEpoch [2], train_loss: 0.00019, val_loss: 0.05193, val_acc: 0.99100\nEpoch [3], train_loss: 0.00004, val_loss: 0.05559, val_acc: 0.99107\nEpoch [4], train_loss: 0.00002, val_loss: 0.05456, val_acc: 0.99148\nEpoch [5], train_loss: 0.00001, val_loss: 0.05643, val_acc: 0.99129\nEpoch [6], train_loss: 0.00001, val_loss: 0.05881, val_acc: 0.99105\nEpoch [7], train_loss: 0.00001, val_loss: 0.06032, val_acc: 0.99124\nEpoch [8], train_loss: 0.00000, val_loss: 0.06221, val_acc: 0.99100\nEpoch [9], train_loss: 0.00000, val_loss: 0.06400, val_acc: 0.99100\nEpoch [10], train_loss: 0.00000, val_loss: 0.06559, val_acc: 0.99124\nEpoch [11], train_loss: 0.00000, val_loss: 0.06715, val_acc: 0.99112\nEpoch [12], train_loss: 0.00000, val_loss: 0.06891, val_acc: 0.99105\nEpoch [13], train_loss: 0.00000, val_loss: 0.07009, val_acc: 0.99105\nEpoch [14], train_loss: 0.00000, val_loss: 0.07176, val_acc: 0.99105\nEpoch [15], train_loss: 0.00000, val_loss: 0.07312, val_acc: 0.99105\nEpoch [16], train_loss: 0.00000, val_loss: 0.07482, val_acc: 0.99117\nEpoch [17], train_loss: 0.00000, val_loss: 0.07580, val_acc: 0.99124\nEpoch [18], train_loss: 0.00000, val_loss: 0.07733, val_acc: 0.99124\nEpoch [19], train_loss: 0.00000, val_loss: 0.07883, val_acc: 0.99117\nEpoch [20], train_loss: 0.00000, val_loss: 0.08222, val_acc: 0.99117\n"
    }
   ],
   "source": [
    "epochs = 20\n",
    "lr = 0.0001\n",
    "\n",
    "history = fit_one_cycle(epochs, lr, model, train_loader, val_loader=val_loader, opt_func=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MnistConvolutionalNeuralNetwork(\n  (connections): Sequential(\n    (0): Sequential(\n      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU()\n      (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU()\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (1): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU()\n      (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU()\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (2): Flatten()\n    (3): Linear(in_features=12544, out_features=256, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=256, out_features=128, bias=True)\n    (6): ReLU()\n    (7): Linear(in_features=128, out_features=32, bias=True)\n    (8): ReLU()\n    (9): Linear(in_features=32, out_features=10, bias=True)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "input_channel = 1\n",
    "hidden_layers = [256, 128, 32]\n",
    "output_size = 10\n",
    "model_2 = MnistConvolutionalNeuralNetwork(input_channel, hidden_layers, output_size)\n",
    "to_device(model_2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_loader = DeviceDataLoader(DataLoader(mnist_train, batch_size=batch_size, shuffle=True), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch [1], train_loss: 0.27799\nEpoch [2], train_loss: 0.05341\nEpoch [3], train_loss: 0.03587\nEpoch [4], train_loss: 0.02833\nEpoch [5], train_loss: 0.02013\nEpoch [6], train_loss: 0.01985\nEpoch [7], train_loss: 0.01370\nEpoch [8], train_loss: 0.01186\nEpoch [9], train_loss: 0.00872\nEpoch [10], train_loss: 0.00807\nEpoch [11], train_loss: 0.01024\nEpoch [12], train_loss: 0.00936\nEpoch [13], train_loss: 0.00667\nEpoch [14], train_loss: 0.00833\nEpoch [15], train_loss: 0.00974\n"
    }
   ],
   "source": [
    "epochs = 15\n",
    "lr = 0.001\n",
    "\n",
    "history = fit_one_cycle(epochs, lr, model_2, full_loader, opt_func=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch [1], train_loss: 0.00072\nEpoch [2], train_loss: 0.00008\nEpoch [3], train_loss: 0.00003\nEpoch [4], train_loss: 0.00001\nEpoch [5], train_loss: 0.00001\nEpoch [6], train_loss: 0.00001\nEpoch [7], train_loss: 0.00000\nEpoch [8], train_loss: 0.00000\nEpoch [9], train_loss: 0.00000\nEpoch [10], train_loss: 0.00000\nEpoch [11], train_loss: 0.00000\nEpoch [12], train_loss: 0.00000\nEpoch [13], train_loss: 0.00000\nEpoch [14], train_loss: 0.00000\nEpoch [15], train_loss: 0.00000\n"
    }
   ],
   "source": [
    "epochs = 15\n",
    "lr = 0.0001\n",
    "\n",
    "history = fit_one_cycle(epochs, lr, model_2, full_loader, opt_func=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([28000, 1, 28, 28])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "mnist_test_data = torch.Tensor(test_df.values.reshape(-1, 1, 28, 28)) / 255.\n",
    "mnist_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DeviceDataLoader(DataLoader(mnist_test_data, batch_size=batch_size, shuffle=False), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.7412, 0.7412, 0.8549, 0.9922, 0.9922],\n        [0.9922, 0.9922, 0.9922, 0.9922, 0.9922],\n        [0.9922, 0.9922, 0.9922, 0.9922, 0.9922],\n        [0.9922, 0.9412, 0.4784, 0.4784, 0.7451],\n        [0.9020, 0.2745, 0.0000, 0.0000, 0.0667]], device='cuda:0')\ntensor([[0.7412, 0.7412, 0.8549, 0.9922, 0.9922],\n        [0.9922, 0.9922, 0.9922, 0.9922, 0.9922],\n        [0.9922, 0.9922, 0.9922, 0.9922, 0.9922],\n        [0.9922, 0.9412, 0.4784, 0.4784, 0.7451],\n        [0.9020, 0.2745, 0.0000, 0.0000, 0.0667]])\n"
    }
   ],
   "source": [
    "for sample in test_loader:\n",
    "    print(sample[0, 0, 15:20, 15:20])\n",
    "    break\n",
    "print(mnist_test_data[0, 0, 15:20, 15:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for sample in test_loader:\n",
    "    _, tmp_preds = torch.max(model_2(sample), dim=1)\n",
    "    preds = preds + list(tmp_preds.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[2, 0, 9, 0, 3]"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "preds = [x.item() for x in preds]\n",
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\n",
    "    'ImageId': range(1, len(mnist_test_data) + 1),\n",
    "    'Label': preds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('self_output_cnn.csv', index=False)"
   ]
  }
 ]
}